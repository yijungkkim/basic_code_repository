{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code establishes connection with snowflake using snowflake.connector and sqlalchemy.\n",
    "The code then:\n",
    "\n",
    "1. executes sql code in python\n",
    "2. loops through the sql code and outputs results \n",
    "3. pushes the final dataframe to snowflake as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as scs\n",
    "from datetime import date, datetime\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection\n",
    "DATABASE = 'database' #update for your own account\n",
    "schema =  'schema' #update for your own account\n",
    "username = 'user' #update for your own account\n",
    "account = 'account' #update for your own account\n",
    " \n",
    "kwargs = {\"authenticator\":  \"externalbrowser\"} #options\n",
    "\n",
    "connection = snowflake.connector.connect(\n",
    "    user       = username,\n",
    "    password   = \"\",\n",
    "    account    = account, **kwargs\n",
    "    )\n",
    "\n",
    "#specify schema\n",
    "cur = connection.cursor()\n",
    "     \n",
    "# define schema\n",
    "cur.execute(f'USE {DATABASE}.{schema}')\n",
    "\n",
    "# engine\n",
    "engine = create_engine(URL(\n",
    "account = 'account', #update for your own account\n",
    "user = 'user', #update for your own account\n",
    "database = 'database', #update for your own account\n",
    "schema = 'schema', #update for your own account\n",
    "authenticator='externalbrowser'))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. execute a sql code in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use connection to execute a query\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "    FROM TABLE \n",
    "    where entry_created_at::date < '2023-1-1'\n",
    "\"\"\"\n",
    "\n",
    "# Use cursor to execute a command\n",
    "cur.execute(f'USE DATABASE') #update for your own account\n",
    "\n",
    "# import to dataframe\n",
    "df_prior = pd.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can read from a text file that includes sql code\n",
    "# open text file in read mode\n",
    "text_file = open(\"./test.sql\", \"r\")\n",
    "\n",
    "# read whole file to a string\n",
    "sql = text_file.read()\n",
    "\n",
    "# close file\n",
    "text_file.close()\n",
    "\n",
    "# execute query\n",
    "for chunk in sql.split(';'):\n",
    "    print(chunk,'\\n'*2)\n",
    "    conn.execute(chunk)\n",
    "\n",
    "# import to dataframe\n",
    "df = pd.read_sql('select * from analytics.cte_pt_elig', con = connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. loops through the sql code and outputs results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store dataframes\n",
    "dataframe_collection = {}\n",
    "\n",
    "# Create a list of simulation dataset dates to merge to the baseline - as of 2023-10, there should be 9\n",
    "simulation_dates = ['2023_01_26', '2023_02_28', '2023_03_28', '2023_04_26', '2023_05_30', '2023_06_08', '2023_07_28', '2023_08_24', '2023_09_28']\n",
    "\n",
    "for variable in simulation_dates:\n",
    "    query_to_execute = query_template.format(variable=variable)\n",
    "    dataframe_collection[f\"df_sim_{variable}\"] = pd.read_sql(query_to_execute, connection)\n",
    "    dataframe_collection[f\"df_sim_{variable}\"].columns = dataframe_collection[f\"df_sim_{variable}\"].columns.str.lower()\n",
    "    dataframe_collection[f\"df_sim_{variable}\"]['id_column'] = dataframe_collection[f\"df_sim_{variable}\"]['id_column'].astype(np.int64)\n",
    "    dataframe_collection[f\"df_sim_{variable}\"]['risk_score'] = dataframe_collection[f\"df_sim_{variable}\"]['risk_score'].astype(float)\n",
    "    dataframe_collection[f\"df_sim_{variable}\"]['metric_value'] = dataframe_collection[f\"df_sim_{variable}\"]['metric_value'].astype(float)\n",
    "\n",
    "for threshold_value in threshold_list:\n",
    "    df_simulation = df_simulation.assign(target_group = np.where(df_simulation['risk_metric_product'] > np.nanpercentile(df_simulation['risk_metric_product'], threshold_value, method='median_unbiased'), 1, 0))\n",
    "    data_subset = df_simulation[df_simulation[\"target_group\"] == 1]\n",
    "    df_previous.columns = df_previous.columns.str.lower()\n",
    "    df_previous['id_column'] = df_previous['id_column'].astype(np.int64)\n",
    "    data_subset['id_column'] = data_subset['id_column'].astype(np.int64)\n",
    "\n",
    "    merged_data = pd.merge(df_previous, data_subset, on='id_column', how='outer', indicator=True)\n",
    "    new_entries = merged_data[(merged_data['_merge'] == 'right_only')].drop(columns=['_merge'])\n",
    "    base_df = new_entries.copy()\n",
    "    base_df['id_column'] = base_df['id_column'].astype(np.int64)\n",
    "    current_percentage = 100 - threshold_value\n",
    "    df_keys = [f\"df_sim_{date}_threshold_{threshold_value}\" for date in simulation_dates]\n",
    "\n",
    "    for variable in simulation_dates:\n",
    "        original_key = f\"df_sim_{variable}\"\n",
    "        threshold_key = f\"df_sim_{variable}_threshold_{threshold_value}\"\n",
    "        df_with_threshold = dataframe_collection[original_key].copy()\n",
    "        df_with_threshold = df_with_threshold.assign(risk_metric_product=df_with_threshold['risk_score'] * df_with_threshold['metric_value'])\n",
    "        df_with_threshold = df_with_threshold.assign(target_group=np.where(df_with_threshold['risk_metric_product'] > np.nanpercentile(df_with_threshold['risk_metric_product'], threshold_value, method='median_unbiased'), 1, 0))\n",
    "        df_with_threshold = df_with_threshold[df_with_threshold['target_group'] == 1]\n",
    "        dataframe_collection[threshold_key] = df_with_threshold\n",
    "        print(f\"Number of rows in {threshold_key}: {df_with_threshold.shape[0]}\")\n",
    "\n",
    "    if 1 <= 9 <= len(df_keys):\n",
    "        for key in df_keys[:9]:         \n",
    "            df_to_add = dataframe_collection[key]\n",
    "            base_df.update(df_to_add)\n",
    "            additional_rows = df_to_add[~df_to_add['id_column'].isin(base_df['id_column'])]\n",
    "            base_df = pd.concat([base_df, additional_rows], ignore_index=True)\n",
    "    else:\n",
    "        print(\"Number out of range!\")\n",
    "    \n",
    "    final_df = base_df.copy()\n",
    "    df_target_group = pd.concat([final_df, pd.get_dummies(final_df['age_group'], prefix='age_group')], axis=1)\n",
    "    boolean_columns = ['age_group_1', 'age_group_2', 'age_group_3', 'age_group_4', 'age_group_5', 'age_group_6'] \n",
    "    for col in boolean_columns:\n",
    "        df_target_group[col] = df_target_group[col].astype(int)\n",
    "    df_summary = df_target_group[['risk_score','metric_value']].describe(include='all')\n",
    "    transposed_summary = df_summary.T[['mean']].round(3)\n",
    "    transposed_summary.loc['total_rows'] = len(final_df)\n",
    "    transposed_summary.loc['data_label'] = f\"\"\"{simulation_date}: Simulation {current_percentage}%\"\"\"\n",
    "    ordered_index = ['total_rows', 'data_label'] + [col for col in transposed_summary.index if col not in ['total_rows', 'data_label']]\n",
    "    transposed_summary = transposed_summary.reindex(ordered_index)\n",
    "    thresholds.append(transposed_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. write the table back to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_uppercase = new_entries.copy()  # This creates a new copy of your original DataFrame\n",
    "df_target_uppercase.columns = df_target_uppercase.columns.str.upper()\n",
    "table_name = f'TARGET_DATA_{current_percentage}'\n",
    "\n",
    "get_ipython().run_cell_magic('time', '', f\"\"\"\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# use database\n",
    "cur.execute(f'USE DATA_WAREHOUSE')\n",
    "\n",
    "rows, columns = df_target_uppercase.shape\n",
    "print(f'Uploading {rows:,} rows and {columns:,} columns to data_warehouse.analytics.{table_name}')\n",
    "\n",
    "output = write_pandas(\n",
    "            conn       = connection,\n",
    "            df         = df_target_uppercase,\n",
    "            table_name = table_name,\n",
    "            database   = 'database',\n",
    "            schema     = 'schema',\n",
    "            quote_identifiers = False,\n",
    "            auto_create_table = True,\n",
    "            overwrite = True\n",
    "            ) \n",
    "print(output)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
